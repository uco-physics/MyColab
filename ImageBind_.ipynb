{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyON0+Ikp0Jv4fd68p3Ria1n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3e2df49c368a4c109182efdf950dfb84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ac856f69879a4bee8ff0cdfeddf35039",
              "IPY_MODEL_15148d86b9624a599733a3a1ecb9ac6b",
              "IPY_MODEL_4b0e74b4c9964057bb3285e57c350cf6"
            ],
            "layout": "IPY_MODEL_01596ba2496a44b2b65695d1356db89b"
          }
        },
        "ac856f69879a4bee8ff0cdfeddf35039": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6c0a18c665d4fceb0461531926102a4",
            "placeholder": "​",
            "style": "IPY_MODEL_fec75201ca7d4dca844d6c75f3c053c7",
            "value": "100%"
          }
        },
        "15148d86b9624a599733a3a1ecb9ac6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4151c79ba194be1afb8536ffbbc7b9d",
            "max": 4803584173,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e762460da383475d8be3fa53f0e05501",
            "value": 4803584173
          }
        },
        "4b0e74b4c9964057bb3285e57c350cf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b1b5c15bc6a445faa71011136e9c71c",
            "placeholder": "​",
            "style": "IPY_MODEL_7e5e864a87f74c118c29959d5606034e",
            "value": " 4.47G/4.47G [00:20&lt;00:00, 224MB/s]"
          }
        },
        "01596ba2496a44b2b65695d1356db89b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6c0a18c665d4fceb0461531926102a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fec75201ca7d4dca844d6c75f3c053c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4151c79ba194be1afb8536ffbbc7b9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e762460da383475d8be3fa53f0e05501": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b1b5c15bc6a445faa71011136e9c71c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e5e864a87f74c118c29959d5606034e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uco-physics/MyColab/blob/main/ImageBind_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfThXdiSeDOX",
        "outputId": "81d880ca-5dc0-4e1b-f838-0a518e54cec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ImageBind'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 58 (delta 15), reused 7 (delta 2), pack-reused 19\u001b[K\n",
            "Unpacking objects: 100% (58/58), 2.63 MiB | 12.60 MiB/s, done.\n",
            "/content/ImageBind\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu113\n",
            "Collecting pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d (from -r requirements.txt (line 5))\n",
            "  Cloning https://github.com/facebookresearch/pytorchvideo.git (to revision 28fe037d212663c6a24f373b94cc5d478c8c1a1d) to /tmp/pip-install-lpaerqgx/pytorchvideo_e2f6935eedf2443dbb7e35a55acf20c0\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/pytorchvideo.git /tmp/pip-install-lpaerqgx/pytorchvideo_e2f6935eedf2443dbb7e35a55acf20c0\n",
            "  Running command git rev-parse -q --verify 'sha^28fe037d212663c6a24f373b94cc5d478c8c1a1d'\n",
            "  Running command git fetch -q https://github.com/facebookresearch/pytorchvideo.git 28fe037d212663c6a24f373b94cc5d478c8c1a1d\n",
            "  Running command git checkout -q 28fe037d212663c6a24f373b94cc5d478c8c1a1d\n",
            "  Resolved https://github.com/facebookresearch/pytorchvideo.git to commit 28fe037d212663c6a24f373b94cc5d478c8c1a1d\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch==1.13 (from -r requirements.txt (line 2))\n",
            "  Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.1/890.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.14.0 (from -r requirements.txt (line 3))\n",
            "  Downloading torchvision-0.14.0-cp310-cp310-manylinux1_x86_64.whl (24.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.13.0 (from -r requirements.txt (line 4))\n",
            "  Downloading torchaudio-0.13.0-cp310-cp310-manylinux1_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm==0.6.7 (from -r requirements.txt (line 6))\n",
            "  Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy (from -r requirements.txt (line 7))\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2022.10.31)\n",
            "Collecting einops (from -r requirements.txt (line 9))\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fvcore (from -r requirements.txt (line 10))\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting decord==0.6.0 (from -r requirements.txt (line 11))\n",
            "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13->-r requirements.txt (line 2)) (4.5.0)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0->-r requirements.txt (line 3)) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0->-r requirements.txt (line 3)) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0->-r requirements.txt (line 3)) (8.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13->-r requirements.txt (line 2)) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13->-r requirements.txt (line 2)) (0.40.0)\n",
            "Collecting av (from pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d->-r requirements.txt (line 5))\n",
            "  Downloading av-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting parameterized (from pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d->-r requirements.txt (line 5))\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Collecting iopath (from pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d->-r requirements.txt (line 5))\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d->-r requirements.txt (line 5)) (3.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->-r requirements.txt (line 7)) (0.2.6)\n",
            "Collecting yacs>=0.1.6 (from fvcore->-r requirements.txt (line 10))\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->-r requirements.txt (line 10)) (6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore->-r requirements.txt (line 10)) (4.65.0)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->-r requirements.txt (line 10)) (2.3.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->-r requirements.txt (line 10)) (0.8.10)\n",
            "Collecting portalocker (from iopath->pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d->-r requirements.txt (line 5))\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0->-r requirements.txt (line 3)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0->-r requirements.txt (line 3)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0->-r requirements.txt (line 3)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0->-r requirements.txt (line 3)) (3.4)\n",
            "Building wheels for collected packages: pytorchvideo, fvcore, iopath\n",
            "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=211208 sha256=971579c557c6b4519dade165edcc36b890389261867c7b167aa90cb0bda382f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/a0/a9/b2f1582cd6198b0425b645bdcce413a15f58d9cc3beee721d0\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61405 sha256=340437fa6000a15be882010d5420e2beccb8e43cf77d1279a73876583829c718\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31531 sha256=3ce6b63c4245fd1807c05707bf0cde665a32e33c2b6d6be55e0d309df0fcd670\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built pytorchvideo fvcore iopath\n",
            "Installing collected packages: av, yacs, portalocker, parameterized, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, ftfy, einops, decord, nvidia-cudnn-cu11, iopath, torch, fvcore, torchvision, torchaudio, pytorchvideo, timm\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.1+cu118\n",
            "    Uninstalling torchvision-0.15.1+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.1+cu118\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.0.1+cu118\n",
            "    Uninstalling torchaudio-2.0.1+cu118:\n",
            "      Successfully uninstalled torchaudio-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.13.0 which is incompatible.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed av-10.0.0 decord-0.6.0 einops-0.6.1 ftfy-6.1.1 fvcore-0.1.5.post20221221 iopath-0.1.10 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 parameterized-0.9.0 portalocker-2.7.0 pytorchvideo-0.1.5 timm-0.6.7 torch-1.13.0 torchaudio-0.13.0 torchvision-0.14.0 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "# パッケージのインストール\n",
        "!git clone https://github.com/facebookresearch/ImageBind\n",
        "%cd ImageBind\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d1VBpgX_e_bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(3) デバイスの準備。\n",
        "GPUとCPUをどちらを使うかの情報です。"
      ],
      "metadata": {
        "id": "a1parrGqfBh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# デバイスの準備\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "CYOz2h7fe_g8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(4) モダリティ (テキスト・画像・音声) の準備。"
      ],
      "metadata": {
        "id": "YFucpKyMfJ7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import data\n",
        "from models.imagebind_model import ModalityType\n",
        "\n",
        "# モダリティ (テキスト・画像・音声) の準備\n",
        "text_list=[\"A dog.\", \"A car\", \"A bird\"]\n",
        "image_paths=[\".assets/dog_image.jpg\", \".assets/car_image.jpg\", \".assets/bird_image.jpg\"]\n",
        "audio_paths=[\".assets/dog_audio.wav\", \".assets/car_audio.wav\", \".assets/bird_audio.wav\"]\n",
        "inputs = {\n",
        "    ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
        "    ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n",
        "    ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Eh5rn-pfKBt",
        "outputId": "0205ab30-e156-4e76-82d1-2cf1b5be4f6a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZaJr9zj8fS3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from models import imagebind_model\n",
        "\n",
        "# モデルの準備\n",
        "model = imagebind_model.imagebind_huge(pretrained=True)\n",
        "model.eval()\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3e2df49c368a4c109182efdf950dfb84",
            "ac856f69879a4bee8ff0cdfeddf35039",
            "15148d86b9624a599733a3a1ecb9ac6b",
            "4b0e74b4c9964057bb3285e57c350cf6",
            "01596ba2496a44b2b65695d1356db89b",
            "c6c0a18c665d4fceb0461531926102a4",
            "fec75201ca7d4dca844d6c75f3c053c7",
            "a4151c79ba194be1afb8536ffbbc7b9d",
            "e762460da383475d8be3fa53f0e05501",
            "7b1b5c15bc6a445faa71011136e9c71c",
            "7e5e864a87f74c118c29959d5606034e"
          ]
        },
        "id": "aeDG5ulyfTA2",
        "outputId": "ca440881-54df-4bc6-aee9-3933c8f08001"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading imagebind weights to .checkpoints/imagebind_huge.pth ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/4.47G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e2df49c368a4c109182efdf950dfb84"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageBindModel(\n",
              "  (modality_preprocessors): ModuleDict(\n",
              "    (vision): RGBDTPreprocessor(\n",
              "      (cls_token): tensor((1, 1, 1280), requires_grad=True)\n",
              "      \n",
              "      (rgbt_stem): PatchEmbedGeneric(\n",
              "        (proj): Sequential(\n",
              "          (0): PadIm2Video()\n",
              "          (1): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
              "        )\n",
              "      )\n",
              "      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
              "        (pos_embed): tensor((1, 257, 1280), requires_grad=True)\n",
              "        \n",
              "      )\n",
              "    )\n",
              "    (text): TextPreprocessor(\n",
              "      (pos_embed): tensor((1, 77, 1024), requires_grad=True)\n",
              "      (mask): tensor((77, 77), requires_grad=False)\n",
              "      \n",
              "      (token_embedding): Embedding(49408, 1024)\n",
              "    )\n",
              "    (audio): AudioPreprocessor(\n",
              "      (cls_token): tensor((1, 1, 768), requires_grad=True)\n",
              "      \n",
              "      (rgbt_stem): PatchEmbedGeneric(\n",
              "        (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10), bias=False)\n",
              "        (norm_layer): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
              "        (pos_embed): tensor((1, 229, 768), requires_grad=True)\n",
              "        \n",
              "      )\n",
              "    )\n",
              "    (depth): RGBDTPreprocessor(\n",
              "      (cls_token): tensor((1, 1, 384), requires_grad=True)\n",
              "      \n",
              "      (depth_stem): PatchEmbedGeneric(\n",
              "        (proj): Conv2d(1, 384, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
              "        (norm_layer): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
              "        (pos_embed): tensor((1, 197, 384), requires_grad=True)\n",
              "        \n",
              "      )\n",
              "    )\n",
              "    (thermal): ThermalPreprocessor(\n",
              "      (cls_token): tensor((1, 1, 768), requires_grad=True)\n",
              "      \n",
              "      (rgbt_stem): PatchEmbedGeneric(\n",
              "        (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
              "        (norm_layer): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
              "        (pos_embed): tensor((1, 197, 768), requires_grad=True)\n",
              "        \n",
              "      )\n",
              "    )\n",
              "    (imu): IMUPreprocessor(\n",
              "      (pos_embed): tensor((1, 251, 512), requires_grad=True)\n",
              "      (cls_token): tensor((1, 1, 512), requires_grad=True)\n",
              "      \n",
              "      (imu_stem): PatchEmbedGeneric(\n",
              "        (proj): Linear(in_features=48, out_features=512, bias=False)\n",
              "        (norm_layer): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (modality_trunks): ModuleDict(\n",
              "    (vision): SimpleTransformer(\n",
              "      (pre_transformer_layer): Sequential(\n",
              "        (0): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (1): EinOpsRearrange()\n",
              "      )\n",
              "      (blocks): Sequential(\n",
              "        (0): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (6): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (7): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (8): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (9): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (10): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (11): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (12): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (13): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (14): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (15): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (16): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (17): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (18): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (19): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (20): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (21): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (22): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (23): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (24): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (25): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (26): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (27): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (28): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (29): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (30): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (31): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (post_transformer_layer): EinOpsRearrange()\n",
              "    )\n",
              "    (text): SimpleTransformer(\n",
              "      (pre_transformer_layer): Sequential(\n",
              "        (0): Identity()\n",
              "        (1): EinOpsRearrange()\n",
              "      )\n",
              "      (blocks): Sequential(\n",
              "        (0): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (6): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (7): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (8): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (9): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (10): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (11): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (12): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (13): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (14): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (15): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (16): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (17): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (18): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (19): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (20): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (21): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (22): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (23): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (post_transformer_layer): EinOpsRearrange()\n",
              "    )\n",
              "    (audio): SimpleTransformer(\n",
              "      (pre_transformer_layer): Sequential(\n",
              "        (0): Identity()\n",
              "        (1): EinOpsRearrange()\n",
              "      )\n",
              "      (blocks): Sequential(\n",
              "        (0): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.009)\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.018)\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.027)\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.036)\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.045)\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (6): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.055)\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (7): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.064)\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (8): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.073)\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (9): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.082)\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (10): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.091)\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (11): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.100)\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (post_transformer_layer): EinOpsRearrange()\n",
              "    )\n",
              "    (depth): SimpleTransformer(\n",
              "      (pre_transformer_layer): Sequential(\n",
              "        (0): Identity()\n",
              "        (1): EinOpsRearrange()\n",
              "      )\n",
              "      (blocks): Sequential(\n",
              "        (0): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (6): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (7): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (8): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (9): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (10): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (11): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (post_transformer_layer): EinOpsRearrange()\n",
              "    )\n",
              "    (thermal): SimpleTransformer(\n",
              "      (pre_transformer_layer): Sequential(\n",
              "        (0): Identity()\n",
              "        (1): EinOpsRearrange()\n",
              "      )\n",
              "      (blocks): Sequential(\n",
              "        (0): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (6): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (7): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (8): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (9): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (10): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (11): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (post_transformer_layer): EinOpsRearrange()\n",
              "    )\n",
              "    (imu): SimpleTransformer(\n",
              "      (pre_transformer_layer): Sequential(\n",
              "        (0): Identity()\n",
              "        (1): EinOpsRearrange()\n",
              "      )\n",
              "      (blocks): Sequential(\n",
              "        (0): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.140)\n",
              "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.280)\n",
              "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.420)\n",
              "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.560)\n",
              "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BlockWithMasking(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (drop_path): DropPath(drop_prob=0.700)\n",
              "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (post_transformer_layer): EinOpsRearrange()\n",
              "    )\n",
              "  )\n",
              "  (modality_heads): ModuleDict(\n",
              "    (vision): Sequential(\n",
              "      (0): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "      (1): SelectElement()\n",
              "      (2): Linear(in_features=1280, out_features=1024, bias=False)\n",
              "    )\n",
              "    (text): SelectEOSAndProject(\n",
              "      (proj): Sequential(\n",
              "        (0): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "        (1): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "      )\n",
              "    )\n",
              "    (audio): Sequential(\n",
              "      (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (1): SelectElement()\n",
              "      (2): Linear(in_features=768, out_features=1024, bias=False)\n",
              "    )\n",
              "    (depth): Sequential(\n",
              "      (0): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (1): SelectElement()\n",
              "      (2): Linear(in_features=384, out_features=1024, bias=False)\n",
              "    )\n",
              "    (thermal): Sequential(\n",
              "      (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (1): SelectElement()\n",
              "      (2): Linear(in_features=768, out_features=1024, bias=False)\n",
              "    )\n",
              "    (imu): Sequential(\n",
              "      (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "      (1): SelectElement()\n",
              "      (2): Dropout(p=0.5, inplace=False)\n",
              "      (3): Linear(in_features=512, out_features=1024, bias=False)\n",
              "    )\n",
              "  )\n",
              "  (modality_postprocessors): ModuleDict(\n",
              "    (vision): Normalize()\n",
              "    (text): Sequential(\n",
              "      (0): Normalize()\n",
              "      (1): LearnableLogitScaling(logit_scale_init=14.285714285714285,learnable=True, max_logit_scale=100)\n",
              "    )\n",
              "    (audio): Sequential(\n",
              "      (0): Normalize()\n",
              "      (1): LearnableLogitScaling(logit_scale_init=20.0,learnable=False, max_logit_scale=100)\n",
              "    )\n",
              "    (depth): Sequential(\n",
              "      (0): Normalize()\n",
              "      (1): LearnableLogitScaling(logit_scale_init=5.0,learnable=False, max_logit_scale=100)\n",
              "    )\n",
              "    (thermal): Sequential(\n",
              "      (0): Normalize()\n",
              "      (1): LearnableLogitScaling(logit_scale_init=10.0,learnable=False, max_logit_scale=100)\n",
              "    )\n",
              "    (imu): Sequential(\n",
              "      (0): Normalize()\n",
              "      (1): LearnableLogitScaling(logit_scale_init=5.0,learnable=False, max_logit_scale=100)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 推論の実行\n",
        "with torch.no_grad():\n",
        "    embeddings = model(inputs)\n",
        "\n",
        "# 結果の確認\n",
        "print(\n",
        "    \"Vision x Text: \",\n",
        "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
        ")\n",
        "print(\n",
        "    \"Audio x Text: \",\n",
        "    torch.softmax(embeddings[ModalityType.AUDIO] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
        ")\n",
        "print(\n",
        "    \"Vision x Audio: \",\n",
        "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.AUDIO].T, dim=-1),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqpL5HWxfhA4",
        "outputId": "551e210d-3ce8-44c5-af89-e568b629d4aa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vision x Text:  tensor([[9.9684e-01, 3.1310e-03, 2.5929e-05],\n",
            "        [5.4495e-05, 9.9993e-01, 2.0353e-05],\n",
            "        [4.4847e-05, 1.3246e-02, 9.8671e-01]], device='cuda:0')\n",
            "Audio x Text:  tensor([[1., 0., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.]], device='cuda:0')\n",
            "Vision x Audio:  tensor([[0.8064, 0.1051, 0.0885],\n",
            "        [0.1284, 0.7205, 0.1511],\n",
            "        [0.0016, 0.0022, 0.9962]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "「画像 x テキスト」「音声 x テキスト」「画像 x 音声」のモダリティの距離が出力されます。1に近いほど類似してることを示します。「dog, car, bird」同士を比較してるため、対角線上に1に近い値が並んでいます。"
      ],
      "metadata": {
        "id": "GyQNNXKnfw80"
      }
    }
  ]
}